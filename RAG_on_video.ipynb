{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install langchain-community\n",
        "!python -m pip install python-dotenv\n",
        "\n",
        "!pip install --upgrade pytube\n",
        "!pip install yt-dlp\n",
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmxWnqq52aYb",
        "outputId": "577461b5-c3d5-4910-ea3d-00e9671d273e",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.9)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.20 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.21)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.91)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain) (3.0.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.9)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.2.21)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.91)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.7->langchain-community) (2.8.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.12->langchain-community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain-community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.12->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.7->langchain-community) (2.20.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.10/dist-packages (1.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OiM5E8ownBRp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv('/content/.env')\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# This is the YouTube video we're going to use.\n",
        "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=ISZCSikwSlI&t=51s\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai\n",
        "!pip install whisper\n",
        "!pip install pytube"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPYJznZM3M7C",
        "outputId": "aa5e38fe-042a-4143-cf1b-768a2e0f9ca6",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.10/dist-packages (0.1.17)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.20 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.2.21)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.35.15)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain-openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain-openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain-openai) (0.1.91)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain-openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain-openai) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.20->langchain-openai) (8.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (2024.7.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.20->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3.0,>=0.2.20->langchain-openai) (3.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.20->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3.0,>=0.2.20->langchain-openai) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.0.7)\n",
            "Requirement already satisfied: whisper in /usr/local/lib/python3.10/dist-packages (1.1.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from whisper) (1.16.0)\n",
            "Requirement already satisfied: pytube in /usr/local/lib/python3.10/dist-packages (15.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fRkX7dA7nBRs"
      },
      "outputs": [],
      "source": [
        "#Setting up the llm model\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-WP7S87nBRs",
        "outputId": "73dce07a-0236-4fcd-e4a7-926eb6c667c0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Kareem Abdul-Jabbar holds the record for the most career points scored in NBA history with a total of 38,387 points.', response_metadata={'token_usage': {'completion_tokens': 29, 'prompt_tokens': 18, 'total_tokens': 47}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-6eab0064-0b0f-4869-959d-5de5c5229dca-0', usage_metadata={'input_tokens': 18, 'output_tokens': 29, 'total_tokens': 47})"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "#test the model which will give an AIMessage instance containing the answer\n",
        "model.invoke(\"Which player have scored the most career points in NBA?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P7YkCJJrnBRu",
        "outputId": "c399635a-eca6-484c-8d1d-47142cb06437",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Kareem Abdul-Jabbar is the player who has scored the most career points in NBA history, with a total of 38,387 points.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "#chaining the AIMessage of the model with an output parser -> StrOutputParser\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "chain = model | parser\n",
        "chain.invoke(\"Which player have scored the most career points in NBA?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "ayFtzaN0nBRv",
        "outputId": "6af58e5f-a6bf-4fc4-e050-aa246b80698f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Human: \\nAnswer the question based on the context below. If you can\\'t\\nanswer the question, reply \"I don\\'t know\".\\n\\nContext: Mary\\'s sister is Susana\\n\\nQuestion: Who is Mary\\'s sister?\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "#provide the model with some context and the question --> see https://python.langchain.com/v0.1/docs/modules/model_io/prompts/quick_start/\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "template = \"\"\"\n",
        "Answer the question based on the context below. If you can't\n",
        "answer the question, reply \"I don't know\".\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "prompt.format(context=\"Mary's sister is Susana\", question=\"Who is Mary's sister?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZwmMmAQanBRw",
        "outputId": "aa260c9c-083a-4251-b9db-182c98113b07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Susana'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#chain the prompt with the model and the output parser -->  https://dashboard.cohere.com/playground/embed to see it more clearly\n",
        "chain = prompt | model | parser\n",
        "chain.invoke({\n",
        "    \"context\": \"Mary's sister is Susana\",\n",
        "    \"question\": \"Who is Mary's sister?\"\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "T63rrhnynBRx"
      },
      "outputs": [],
      "source": [
        "#Combining chains, here we can combine different chains to create more complex workflows. Let's create a second chain that translates the answer from the first chain into a different language\n",
        "#a new prompt template for the translation chain\n",
        "translation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"Translate {answer} to {language}\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tPRKRNXInBRx",
        "outputId": "f57b4d93-e791-497a-d89d-11173809a6d4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'María tiene una hermana.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "#creation of  a new translation chain that combines the result from the first chain with the translation prompt\n",
        "from operator import itemgetter\n",
        "\n",
        "translation_chain = (\n",
        "    {\"answer\": chain, \"language\": itemgetter(\"language\")} | translation_prompt | model | parser\n",
        ")\n",
        "\n",
        "translation_chain.invoke(\n",
        "    {\n",
        "        \"context\": \"Mary's sister is Susana. She doesn't have any more siblings.\",\n",
        "        \"question\": \"How many sisters does Mary have?\",\n",
        "        \"language\": \"Spanish\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHtVpAZsnBRy",
        "outputId": "798b6e4a-c055-4806-9a86-9abe5b4d0988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned URL: https://www.youtube.com/watch?v=ISZCSikwSlI\n",
            "Attempting to download audio using yt-dlp...\n",
            "Downloaded audio file path: ISZCSikwSlI.mp3\n",
            "Loading Whisper model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 139M/139M [00:01<00:00, 118MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcribing audio...\n",
            "Transcription completed and saved to transcription.txt\n"
          ]
        }
      ],
      "source": [
        "#Transcribing the video\n",
        "import os\n",
        "import tempfile\n",
        "import whisper\n",
        "from pytube import YouTube\n",
        "from pytube.exceptions import RegexMatchError, VideoUnavailable, PytubeError\n",
        "import yt_dlp\n",
        "\n",
        "# Function to clean the YouTube URL (remove timestamp and other parameters)\n",
        "def clean_youtube_url(url):\n",
        "    if \"&\" in url:\n",
        "        return url.split(\"&\")[0]\n",
        "    return url\n",
        "\n",
        "# Clean the YouTube URL\n",
        "cleaned_url = clean_youtube_url(YOUTUBE_VIDEO)\n",
        "print(f\"Cleaned URL: {cleaned_url}\")\n",
        "\n",
        "# Check if the transcription file already exists\n",
        "transcription_file_path = \"/content/transcription.txt\"\n",
        "\n",
        "if not os.path.exists(transcription_file_path):\n",
        "    try:\n",
        "        print(\"Attempting to download audio using yt-dlp...\")\n",
        "\n",
        "        # yt-dlp to download audio\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'outtmpl': '%(id)s.%(ext)s',\n",
        "            'postprocessors': [{\n",
        "                'key': 'FFmpegExtractAudio',\n",
        "                'preferredcodec': 'mp3',\n",
        "                'preferredquality': '192',\n",
        "            }],\n",
        "            'quiet': True\n",
        "        }\n",
        "\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            info_dict = ydl.extract_info(cleaned_url, download=True)\n",
        "            audio_file = ydl.prepare_filename(info_dict).replace('.webm', '.mp3')\n",
        "\n",
        "        print(f\"Downloaded audio file path: {audio_file}\")\n",
        "\n",
        "        if not os.path.exists(audio_file):\n",
        "            raise Exception(\"Failed to download the audio file.\")\n",
        "\n",
        "        # Load the base model\n",
        "        print(\"Loading Whisper model...\")\n",
        "        whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "        print(\"Transcribing audio...\")\n",
        "        transcription = whisper_model.transcribe(audio_file, fp16=False)[\"text\"].strip()\n",
        "\n",
        "        with open(transcription_file_path, \"w\") as file:\n",
        "            file.write(transcription)\n",
        "        print(\"Transcription completed and saved to transcription.txt\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "else:\n",
        "    print(\"Transcription file already exists.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "AWJQNsBYnBRz",
        "outputId": "918373fb-6bb0-47ae-db66-070f0c97587c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"This is the Peter Pan story, roughly speaking. Peter Pan is this magical boy. Pan means the God of everything, roughly speaking, right? And so it's not an accident that he has the name Pan. And he's t\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "#read the transcription\n",
        "with open(\"transcription.txt\") as file:\n",
        "    transcription = file.read()\n",
        "\n",
        "transcription[:200]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "yt4CB0TTnBRz",
        "outputId": "c56b2ff9-b39d-403e-838f-19621239c614",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntry:\\n    chain.invoke({\\n        \"context\": transcription,\\n        \"question\": \"Is reading papers a good idea?\"\\n    })\\nexcept Exception as e:\\n    print(e)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "#If we try to invoke the chain using the transcription as context, the model may return an error because the context is too long. LLMs support limitted context sizes.\n",
        "\"\"\"\n",
        "try:\n",
        "    chain.invoke({\n",
        "        \"context\": transcription,\n",
        "        \"question\": \"Is reading papers a good idea?\"\n",
        "    })\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "jo-tyDo_nBR0",
        "outputId": "ff6a4211-7d62-4c88-94f2-dd6977fa22ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'transcription.txt'}, page_content=\"This is the Peter Pan story, roughly speaking. Peter Pan is this magical boy. Pan means the God of everything, roughly speaking, right? And so it's not an accident that he has the name Pan. And he's the boy that won't grow up. And he's magical. Well that's because children are magical. They can be anything. They're nothing but potential. And Peter Pan doesn't want to give that up. Why? He's got some adults around him, but the main adult is Captain Hook. Well, who the hell wants to grow up to be Captain Hook? First of all, you've got a hook. Second, you're a tyrant. And third, you're chased by the dragon of chaos with a clock in its stomach, right? The crocodile. It's already got a piece of you. Well, that's what happens when you get older. Time has already got a piece of you. And eventually, it's got a taste for you. And eventually, it's going to eat you. And so Hook is so traumatized by that that he can't help but be a tyrant. And then Peter Pan looks at traumatized Hook and says, well, no, I'm not sacrificing my childhood for that. So that's fine. Except he ends up king of lost boys. In Neverland, well, Neverland doesn't exist. And who the hell wants to be king of the lost boys? And he also sacrifices the possibility that he'll help a real relationship with a woman. Because that's Wendy, right? And she's kind of conservative, middle class, London dwelling girl. She wants to grow up and have kids and have a life. She accepts her mortality. She accepts her maturity. Peter Pan has to contend himself with tinkerbell. She doesn't even exist. She's like the fairy of porn. She doesn't exist. She's the substitute for the real thing. But the dichotomy that you're talking about, it's very tricky because there's a sacrificial element in maturation, right? You have to sacrifice the plural potentiality of childhood for the actuality of a frame. And the question is, well, why would you do that? Well, one reason is it happens to you whether you do it or not. You can either choose your damn limitation or you can let it take you unaware when you're 30 or even worse when you're 40. And then that is not a happy day. You see, I see people like this. And I think it's more and more common in our culture, as people can put off mat maturity without suffering in immediate penalty. But all that happens is the penalty accrues. And then when it finally hits, it just walks you because when you're 25, you could be an idiot. It's no problem. Even when you're out in a job search, it's like, well, you don't have any experience and you're kind of clueless. It's, yeah, yeah, you're young. It's no problem. That's what young people are like, but they're full of potential. OK, well, now you're the same person at 30. It's like people aren't so thrilled about you at that point. It's like, what the hell have you been doing for the last 10 years? Well, I'm just as clueless as I was when I was 22. It's, yeah, but you're not 22. You're an old infant, right? And that's an ugly thing, an old infant. So part of the reason you choose your damn sacrifice because the sacrifice is inevitable. But at least you get to choose it. And then there's something that's even more complex than that. In some sense, is that the problem with being a child is that all you are is potential. And it's really low resolution. You could be anything, but you're not anything. So then you go and you adopt an apprenticeship, roughly speaking. And then you become at least you become something. And when you're something, that makes the world open up to you again. Like if you're a really good plumber, then you end up in far more than a plumber, right? You end up being a good employer. If you're a really good plumber, well, then you have some employees. You run a business. You train some other people. You enlarge their lives. You're kind of a pillar of the community. You have your family. Once you pass through that narrow training period, which narrows you and constricts you and develops you at the same time, then you can come out the other end with a bunch of new possibility at hand. And you talked about that. You thought that part of the proper path of development in the last half of life was to rediscover the child that you left behind as you were apprenticing. And so then you get to be something and regain that potential at the same time. Very, very smart. Well, he was very, very smart. That's very wise thing to know. Sacrifice. You get to pick your damn sacrifice. That's all. Don't get to not make one. You're sacrificial whether you want to be or not. That's a good thing to know as well.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#splitting the transcription into smaller chunks. We can then invoke the model using only the relevant chunks to answer a particular question\n",
        "\n",
        "#loading the transcription in memory\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "\n",
        "loader = TextLoader(\"transcription.txt\")\n",
        "text_documents = loader.load()\n",
        "text_documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "xULrbW9KnBR0",
        "outputId": "bd8023c8-756c-4236-f7af-ab1b4df6979e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'transcription.txt'}, page_content='This is the Peter Pan story, roughly speaking. Peter Pan is this magical boy. Pan means the God of'),\n",
              " Document(metadata={'source': 'transcription.txt'}, page_content=\"means the God of everything, roughly speaking, right? And so it's not an accident that he has the\"),\n",
              " Document(metadata={'source': 'transcription.txt'}, page_content=\"that he has the name Pan. And he's the boy that won't grow up. And he's magical. Well that's\"),\n",
              " Document(metadata={'source': 'transcription.txt'}, page_content=\"Well that's because children are magical. They can be anything. They're nothing but potential. And\"),\n",
              " Document(metadata={'source': 'transcription.txt'}, page_content=\"but potential. And Peter Pan doesn't want to give that up. Why? He's got some adults around him,\")]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "#splitting the transcription into chunks of 100 characters with an overlap of 20 characters and display the first few chunks\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
        "text_splitter.split_documents(text_documents)[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8XdD1ifwnBR1"
      },
      "outputs": [],
      "source": [
        "#now with 1000 Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "documents = text_splitter.split_documents(text_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KKcDr1RVnBR1",
        "outputId": "e26c46db-8fe3-40c8-cbcb-d05e9ef661a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding length: 1536\n",
            "[-0.001359404530376196, -0.03437049686908722, -0.0114255640655756, 0.001291395165026188, -0.02616560459136963, 0.009161713533103466, -0.015621816739439964, 0.0018229621928185225, -0.011800787411630154, -0.03324482589960098]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "For a particular question, we need to find the relevant chunks from the transcription to send to the model.\n",
        "Here is where the idea of embeddings comes into play\n",
        "An embedding is a mathematical representation of the semantic meaning of a word, sentence, or document.\n",
        "It's a projection of a concept in a high-dimensional space.\n",
        "Embeddings have a simple characteristic: The projection of related concepts will be close to each other,\n",
        "while concepts with different meanings will lie far away. --> cohere playground\n",
        "\n",
        "To provide with the most relevant chunks, we can use the embeddings of the question and the chunks of the transcription to compute the similarity between them.\n",
        "We can then select the chunks with the highest similarity to the question and use them as the context for the model\n",
        "\"\"\"\n",
        "#generating embeddings\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "embedded_query = embeddings.embed_query(\"Who is Mary's sister?\")\n",
        "\n",
        "print(f\"Embedding length: {len(embedded_query)}\")\n",
        "print(embedded_query[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "VYc_ffGgnBR2"
      },
      "outputs": [],
      "source": [
        "#generating the embeddings for two different sentences\n",
        "sentence1 = embeddings.embed_query(\"Mary's sister is Susana\")\n",
        "sentence2 = embeddings.embed_query(\"Pedro's mother is a teacher\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "1ZwB6uqHnBR4",
        "outputId": "b6499850-6ad7-472a-943d-c7764b8cd7aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.9172681467301314, 0.7680251090423714)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#computing the similarity between the query and each of the two sentences. The closer the embeddings are, the more similar the sentences will be.\n",
        "#using cosine similarity to calculate the similarity between the query and each of the sentences -->\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "query_sentence1_similarity = cosine_similarity([embedded_query], [sentence1])[0][0]\n",
        "query_sentence2_similarity = cosine_similarity([embedded_query], [sentence2])[0][0]\n",
        "\n",
        "query_sentence1_similarity, query_sentence2_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install docarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQd7Sd9lBYAU",
        "outputId": "08e79301-70d7-4307-8cfc-b6aef3fe3519",
        "collapsed": true
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting docarray\n",
            "  Downloading docarray-0.40.0-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.2/270.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from docarray) (1.25.2)\n",
            "Requirement already satisfied: orjson>=3.8.2 in /usr/local/lib/python3.10/dist-packages (from docarray) (3.10.6)\n",
            "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/lib/python3.10/dist-packages (from docarray) (2.8.2)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (13.7.1)\n",
            "Collecting types-requests>=2.28.11.6 (from docarray)\n",
            "  Downloading types_requests-2.32.0.20240712-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from docarray) (0.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10.8->docarray) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=13.1.0->docarray) (2.16.1)\n",
            "Requirement already satisfied: urllib3>=2 in /usr/local/lib/python3.10/dist-packages (from types-requests>=2.28.11.6->docarray) (2.0.7)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->docarray) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->docarray) (0.1.2)\n",
            "Installing collected packages: types-requests, docarray\n",
            "Successfully installed docarray-0.40.0 types-requests-2.32.0.20240712\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "29kUjg42nBR5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aeca9c2-7af0-4243-fe18-68751c484e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pydantic/_migration.py:283: UserWarning: `pydantic.error_wrappers:ValidationError` has been moved to `pydantic:ValidationError`.\n",
            "  warnings.warn(f'`{import_path}` has been moved to `{new_location}`.')\n"
          ]
        }
      ],
      "source": [
        "#setting up a vector store to store document chunks, their embeddings, and perform similarity searches at scale\n",
        "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
        "\n",
        "vectorstore1 = DocArrayInMemorySearch.from_texts(\n",
        "    [\n",
        "        \"Mary's sister is Susana\",\n",
        "        \"John and Tommy are brothers\",\n",
        "        \"Patricia likes white cars\",\n",
        "        \"Pedro's mother is a teacher\",\n",
        "        \"Lucia drives an Audi\",\n",
        "        \"Mary has two siblings\",\n",
        "    ],\n",
        "    embedding=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "AFqi3Lj5nBR6",
        "outputId": "408f02d0-925a-4340-8f51-ecef2f4da2cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Document(page_content=\"Mary's sister is Susana\"), 0.9172681550033172),\n",
              " (Document(page_content='Mary has two siblings'), 0.9045628481161785),\n",
              " (Document(page_content='John and Tommy are brothers'), 0.8015500435454905)]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "#quering the vector store to find the most similar embeddings to a given query\n",
        "vectorstore1.similarity_search_with_score(query=\"Who is Mary's sister?\", k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BJXliYHVnBR7",
        "outputId": "2f9d4f86-c01e-4009-d6f5-09fc09aaa705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content=\"Mary's sister is Susana\"),\n",
              " Document(page_content='Mary has two siblings'),\n",
              " Document(page_content='John and Tommy are brothers'),\n",
              " Document(page_content=\"Pedro's mother is a teacher\")]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "# Connecting the vector store to the chain --> to use the vector store to find the most relevant chunks from the transcription to send to the model\n",
        "#The retriever will run a similarity search in the vector store and return the most similar documents back to the next step in the chain -->https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/\n",
        "retriever1 = vectorstore1.as_retriever() #We can get a retriever directly from the vector store we created before\n",
        "retriever1.invoke(\"Who is Mary's sister?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"create a map with the two inputs by using the RunnableParallel and RunnablePassthrough classes to pass the context and question\n",
        "to the prompt as a map with the keys \"context\" and \"question.\"\n",
        "the retriever will find the chunks to use as the context to answer the question.\n",
        "- RunnableParallel --> https://python.langchain.com/v0.1/docs/expression_language/primitives/parallel/\n",
        "- RunnablePassthrough --> https://python.langchain.com/v0.1/docs/expression_language/primitives/passthrough/\n",
        "\"\"\"\n",
        "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
        "\n",
        "setup = RunnableParallel(context=retriever1, question=RunnablePassthrough())\n",
        "setup.invoke(\"What color is Patricia's car?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIoeUj6dk8mc",
        "outputId": "e147cbd9-9f0c-4c3e-ec5e-4dbc1ce9f917"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'context': [Document(page_content='Patricia likes white cars'),\n",
              "  Document(page_content='Lucia drives an Audi'),\n",
              "  Document(page_content=\"Pedro's mother is a teacher\"),\n",
              "  Document(page_content=\"Mary's sister is Susana\")],\n",
              " 'question': \"What color is Patricia's car?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "APp3q-ntnBR-",
        "outputId": "97e08d31-25ee-4487-adf0-90d2753908cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'White'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "#add the setup map to the chain\n",
        "chain = setup | prompt | model | parser\n",
        "chain.invoke(\"What color is Patricia's car?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ToRrKIjAnBR_",
        "outputId": "2dcf85b9-940a-4c1a-dac6-73456e39aecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lucia drives an Audi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "#invoking the whain with another example\n",
        "chain.invoke(\"What car does Lucia drive?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "6u4CUfcFnBR_"
      },
      "outputs": [],
      "source": [
        "#Loading transcription into the vector store so creating a new vector store using the chunks from the video transcription\n",
        "vectorstore2 = DocArrayInMemorySearch.from_documents(documents, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "4YROdFXZnBSA",
        "outputId": "827240be-69ae-4399-c53c-f21006f09d7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'An infant is a young child in the early stages of life, typically under the age of one year.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "#new chain using the correct vector store with a different equivalent syntax to specify the RunnableParallel portion of the chain\n",
        "chain = (\n",
        "    {\"context\": vectorstore2.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "chain.invoke(\"what is an infant?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_pinecone"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqmnVgPbFVl2",
        "outputId": "f766e123-191f-4ab1-f16d-51debd81d3e5",
        "collapsed": true
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_pinecone\n",
            "  Downloading langchain_pinecone-0.1.2-py3-none-any.whl (8.5 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.1.52 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (0.2.21)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_pinecone) (1.25.2)\n",
            "Collecting pinecone-client<5,>=3.2.2 (from langchain_pinecone)\n",
            "  Downloading pinecone_client-4.1.2-py3-none-any.whl (216 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.4/216.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.1.91)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.1.52->langchain_pinecone) (8.5.0)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (2024.7.4)\n",
            "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone-client<5,>=3.2.2->langchain_pinecone)\n",
            "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
            "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (4.12.2)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone-client<5,>=3.2.2->langchain_pinecone) (2.0.7)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.10.6)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.1.52->langchain_pinecone) (2.20.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.1.52->langchain_pinecone) (3.7)\n",
            "Installing collected packages: pinecone-plugin-interface, pinecone-client, langchain_pinecone\n",
            "Successfully installed langchain_pinecone-0.1.2 pinecone-client-4.1.2 pinecone-plugin-interface-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "6wuqGvXgnBSA"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "So far we've used an in-memory vector store\n",
        "Setting up Pinecone to use it as a vector store that can handle large amounts of data and perform similarity searches at scale\n",
        "create a Pinecone account, set up an index, get an API key, and set it as an environment variable (.env) PINECONE_API_KEY\n",
        "\"\"\"\n",
        "#load the transcription documents into Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "index_name = \"rag-on-video-index\"\n",
        "\n",
        "pinecone = PineconeVectorStore.from_documents(\n",
        "    documents, embeddings, index_name=index_name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "L-BuiLWNnBSB",
        "outputId": "6a3d6382-ddfc-4c47-aeda-ef3353092041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'transcription.txt'}, page_content=\"22. You're an old infant, right? And that's an ugly thing, an old infant. So part of the reason you choose your damn sacrifice because the sacrifice is inevitable. But at least you get to choose it. And then there's something that's even more complex than that. In some sense, is that the problem with being a child is that all you are is potential. And it's really low resolution. You could be anything, but you're not anything. So then you go and you adopt an apprenticeship, roughly speaking. And then you become at least you become something. And when you're something, that makes the world open up to you again. Like if you're a really good plumber, then you end up in far more than a plumber, right? You end up being a good employer. If you're a really good plumber, well, then you have some employees. You run a business. You train some other people. You enlarge their lives. You're kind of a pillar of the community. You have your family. Once you pass through that narrow training period,\"),\n",
              " Document(metadata={'source': 'transcription.txt'}, page_content=\"you do it or not. You can either choose your damn limitation or you can let it take you unaware when you're 30 or even worse when you're 40. And then that is not a happy day. You see, I see people like this. And I think it's more and more common in our culture, as people can put off mat maturity without suffering in immediate penalty. But all that happens is the penalty accrues. And then when it finally hits, it just walks you because when you're 25, you could be an idiot. It's no problem. Even when you're out in a job search, it's like, well, you don't have any experience and you're kind of clueless. It's, yeah, yeah, you're young. It's no problem. That's what young people are like, but they're full of potential. OK, well, now you're the same person at 30. It's like people aren't so thrilled about you at that point. It's like, what the hell have you been doing for the last 10 years? Well, I'm just as clueless as I was when I was 22. It's, yeah, but you're not 22. You're an old\"),\n",
              " Document(metadata={'source': 'transcription.txt'}, page_content=\"Hook and says, well, no, I'm not sacrificing my childhood for that. So that's fine. Except he ends up king of lost boys. In Neverland, well, Neverland doesn't exist. And who the hell wants to be king of the lost boys? And he also sacrifices the possibility that he'll help a real relationship with a woman. Because that's Wendy, right? And she's kind of conservative, middle class, London dwelling girl. She wants to grow up and have kids and have a life. She accepts her mortality. She accepts her maturity. Peter Pan has to contend himself with tinkerbell. She doesn't even exist. She's like the fairy of porn. She doesn't exist. She's the substitute for the real thing. But the dichotomy that you're talking about, it's very tricky because there's a sacrificial element in maturation, right? You have to sacrifice the plural potentiality of childhood for the actuality of a frame. And the question is, well, why would you do that? Well, one reason is it happens to you whether you do it or not.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "#testing the similarity search on pinecone\n",
        "pinecone.similarity_search(\"What is the problem with being a child?\")[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "wUyg1wtZnBSC",
        "outputId": "135462de-4f1f-4135-ebd4-b4b982c982df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"The problem with being a child is that all you are is potential, and it's low resolution. You could be anything, but you're not anything until you adopt an apprenticeship and become something.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "#setting up the new chain using Pinecone as the vector store\n",
        "chain = (\n",
        "    {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | parser\n",
        ")\n",
        "\n",
        "chain.invoke(\"What is the problem with being a child?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}